
[[high-availability]]
==  High Availability - Clusters
{inrmonly}

[[high-availability-introduction]]
=== Introduction

High Availability Clusters (HA-C) is a set of features designed to improve the availability of a {nxrm}
deployment. This chapter explains why you would use HA-LC, how it works, supported architectures for deploying 
HC-LC, and procedures for setting it up.

[[high-availability-deciding-to-use-ha-c]]
=== Making the Decision to Use HA-C

Managing three nodes is more complex than managing one. In particular, backup and restore procedures are more
complex, as is the process for upgrading to newer versions of {rm}.

HA-C is not intended to improve the throughput or performance of an {rm} deployment, nor is it a replacement for
traditional backup procedures. Consider using HA-C if your need for increased uptime outweighs the additional
operational complexity.

[[high-availability-how-it-works]]
=== How It Works

In a typical HA-C deployment, three {rm} nodes sit behind a load balancer, which divides incoming requests between
them. The nodes operate in an active/active configuration, meaning that both reads and writes (e.g. publishing new
components) can be made to any of the three nodes.

Internally, {rm} uses http://orientdb.com/orientdb/[OrientDB], a clustered database. Orient replicates state
between the nodes using distributed transactions. Each node votes on whether to accept a transaction, and if a
majority quorum of nodes respond and report success, the transaction is accepted. In this way, if one node
fails, anything written to it is guaranteed to be on at least one other node. The remaining two nodes still form a
a majority, and can continue to accept writes until the failed node is restored and rejoins the cluster.

[[high-availability-what-is-replicated]]
==== What's Replicated?

{nxrm} uses OrientDB to replicate:

* all configuration which is modified through the {nxrm} user interface
* metadata about the components stored in {rm}

Some state is *not* replicated:

* startup configuration files for each instance
* binaries for the components hosted or proxied within {rm}

Any changes you make to the startup configuration need to be made on each node separately. This includes (but is not
limited to) {rm} configuration properties, JVM settings, Jetty configuration overrides, and Hazelcast configuration.

In addition, {rm} does not replicate the binary artifacts themselves, as these are stored in file system-based blob
store. For an HA-C deployment, administrators must provide a file system which is accessible to all three nodes
and which meets their availability requirements.

This is illustrated in the following diagram:

// https://docs.google.com/drawings/d/1ODAxSyOUw7IX6gmIvMasjzRXh_mLy545C0FTvGrO0t8/edit

image::figs/web/high-availability-arch-simple.png[scale=70]

[[high-availability-prepare]]
==== Preparing an HA Environment

NOTE: Some application settings must be made on each node. See <<high-availability-environment >> for details
on what's needed to properly configure an individual node for HA.

{nxrm} HA clusters should be configured with three nodes for normal operation. Cluster nodes vote on each
change that passes through the cluster, and accept them if a majority quorum of nodes can be contacted. As
components are uploaded or modified, metadata about those components are synchronized across the cluster.
Component binaries sit in a shared, on-disk blob store accessible to all three nodes.

Nodes can run on physical or virtual servers, Docker containers, or cloud services like <<high-availability-aws,AWS>>.

Whichever combination of servers you choose, each node must be visible to each other so the cluster can form.
In the cluster itself, the nodes must share the same network so that latency doesn't negatively impact performance.

[[high-availability-requirements]]
=== Requirements for an HA Environment

Before you begin, you should have a basic understanding of the required tools and software to get your
HA environment configured. Hereâ€™s a list of the most important things you should have:

* An installation of {pro} HA
** If you have an existing {pro} installation without HA upgrade to an HA-supported version of {pro}, from
the instructions in the https://support.sonatype.com/hc/en-us/articles/115000350007[support article]
* A load balancer, such as NGINX or Apache HTTP
* Shared storage, for maintaining repository data shared among the nodes.

==== Network Preparation

The nodes in a {nxrm} cluster need to be able to communicate with each other over TCP/IP. For cluster
communications, {nxrm} will use a range of ports starting with 5701. Each node in the {nxrm} cluster will require
that an extra port is available, the extra ports used by {nxrm} will be bound sequentially by default.

For example, in a three-node cluster, each of the nodes in the cluster will need to have ingress opened on ports
`5701`, `5702`, and `5703`. The nodes will use ephemeral ports for outbound (egress) communications. If outbound or
inbound network communications need to be customized and may be blocked by a firewall or other network appliance,
the ports used for cluster communications can be customized in the Hazelcast configuration.

TIP: For more information on customizing Hazelcast and the ports it uses, please see the documentation for
http://docs.hazelcast.org/docs/3.6/manual/html-single/index.html#setting-up-clusters[Hazelcast cluster configuration].

The nodes in a {nxrm} cluster will also replicate database transactions among the nodes in the cluster. The database
requires ports `2424` and `2434` be open for ingress and egress to each of the rest of the nodes in the cluster.

==== Node Discovery

In the default configuration, {nxrm} uses multicast to discover other {nxrm} nodes. This is done to simplify cluster
configuration. However, multicast may not work reliably in all network environments. To test multicast connectivity
between nodes, we recommend using https://iperf.fr/[iPerf]. IPerf is available for Windows, Linux and Mac OSX.

To test multicast connectivity between two nodes, each node will need to have iPerf installed (note: iPerf3 does not
support multicast client testing). During testing, one node will act as the "server" while the other node acts as the
"client." This is important during testing, and further, we suggest that each node be tested as a server and as a client
to ensure proper two-way multicast communication.

*Testing multicast from the server side.*
----
iperf -s -B 224.2.2.3 -p 54327 -i 1
----

*Testing multicast from the client side.*
----
iperf -c 224.2.2.3 -p 54327 -u -i 1
----

The address and port `224.2.2.3:54327` was chosen because this is the default port and address that will be used
by {nxrm} during the node discovery. When the client is able to successfully connect and communicate with the server
node, the client will output a set of brief diagnostic messages indicating how much data was sent and received. The
following is a sample of the output from a client in a successful session:

.Sample Output
....
Client connecting to 224.2.2.3, UDP port 54327
Sending 1470 byte datagrams
Setting multicast TTL to 1
UDP buffer size:  208 KByte (default)

[  3] local 10.10.0.102 port 33743 connected with 224.2.2.3 port 54327
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  3.0- 4.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  4.0- 5.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  5.0- 6.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  6.0- 7.0 sec   129 KBytes  1.06 Mbits/sec
[  3]  7.0- 8.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  8.0- 9.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  9.0-10.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec
[  3] Sent 893 datagrams
....

The advantage of the using multicast for {nxrm} node discovery is that nodes can be added and removed from the
cluster without cluster administrators needing to perform configuration or configuration changes. However, routers
may not be able to route multicast requests properly between subnets, or multicast may be disabled altogether.
In these situations the cluster configuration can be done manually. To manually configure your cluster, please
see the documentation for
http://docs.hazelcast.org/docs/3.6/manual/html-single/index.html#setting-up-clusters[Hazelcast cluster configuration].

[[high-availability-storage]]
==== Storing Critical Data

The core of an HA environment is to have reliable, fault-tolerant storage. The solution is to utilize a shared
storage network to maintain repository data and configuration. To do this create storage locations:

* for each node, including their data directories and exported databases
* where all nodes can shared component binary information as a common blob store access point.

A shared-disk file system is mandatory for your clustered nodes. So on your network, make sure they can see
the blob store on a highly-available file system of your choice.

The clustered nodes share component metadata and repository manager configuration amongst themselves.

////
this section, add to verifying node connection
From the user interface the most accessible way to view active nodes is from the 'Nodes' screen, in the
'Adminstration' menu. It displays all clustered nodes in a table. In this table, notice a distinguishable node
listed as 'true'. The 'true' value indicates that you're viewing your local node from the client (e.g. browser).
Conversely, all values listed as 'false' imply the additional, synchronized nodes are configured to their
own servers.
////

[[high-availability-backup]]
==== Backing up your HA Cluster

Backup for HA uses the same concepts described in the <<backup-restore,Backup and Restore chapter>>. The notable
exception is that your local cluster has shared storage. So, when configuring the 'Export configuration & metadata
for backup' scheduled task choose a node for the backup to run against.

This task, only exports data. But itâ€™s necessary to configure it when backing up and restoring your cluster if you
want to roll back its current state to an earlier state, such as in the case of failure of the underlying
infrastructure.

While the scheduled task runs, all nodes in the cluster become read-only. This is done to preserve data integrity.
While in read-only mode, attempts to publish components or make changes to repository manager configuration will
fail.

{nxrm} will automatically release the read-only status when the backup task completes. Schedule the backup task for
a time of day that usually has low to no activity to minimize client errors


[[high-availability-restore]]
==== Restoring your HA Cluster

Similar to the concepts outlined in <<backup>>, you can restore exported configuration and metadata in your HA
environment. You are required to choose a node from which you desire to restore OrientDB database contents. Then,
follow the steps in detail from <<backup-restore>>. You will want to shut down all other nodes in the cluster
during the restoration process, bringing them back into the cluster after the restoration process has been
completed.

[[high-availability-methods]]
=== Methods to Configure a Cluster

When you're ready to set up a cluster, consider these two methods:

* If you're setting up a new cluster, skip to <<high-availability-new,Using a Fresh Installation of {pro}>>.
* If you're converting an existing server into a cluster, skip to <<high-availability-existing,Converting an
Existing Instance of {pro} into a Cluster>>.

[[high-availability-new]]
==== Using a Fresh Installation of {pro}

To set up a fresh installation of {pro} for high availability:

1. Download {pro}, supported with high availability.
2. Install two additional {pro} instances on different hosts to establish three nodes.
3. Configure the blob store in the first node to an external location where the second and third nodes can
access it.

[[high-availability-existing]]
==== Converting an Existing Instance of {pro} into a Cluster

If you have an existing version pre-HA installation of {pro} 3, you can convert it into a cluster. Follow the
steps below to synchronize it with the second and third nodes.

1. Stop running the existing repository manager.
2. Follow the steps in the https://support.sonatype.com/hc/en-us/articles/231723267[support article] to
upgrade the repository manager to a version that supports high availability.
3. Copy a second and third instance of the newly upgraded repository manager to establish three nodes.
4. Configure the blob store in the first node to an external location where the second and third can access it.
5. <<high-availability-nodes,Enable>> high availability on the three nodes.
6. Start the first node and wait for its start-up sequence to complete, then start the second and third
nodes to form the cluster.

[[high-availability-move]]
==== Moving Blob Stores from an Existing Instance of {pro}

Your existing node may contain blob stores created before initiating HA. These blob stores will only be part of
the existing node, not the HA cluster. So to utilize them in HA, you must relocate them to the shared location
planned for your HA environment. To do so, follow the steps in the
https://support.sonatype.com/hc/en-us/articles/235816228[support article]. After completing the steps, refer to
step 3 in <<high-availability-existing>> to complete HA set up.

[[high-availability-blob-store]]
==== Configuring a Node to Share a Blob Store

NOTE: Configuring a single node to share an entire `sonatype-work` directory will undermine HA configuration,
and might cause functional errors in the cluster. As mentioned in <<high-availability-storage >>, configure
the nodes to share access to components, instead.

To configure a single node sharing blob store access among new nodes:

1. Create a directory in an external location.
2. Start your primary repository manager.
3. Choose a 'Name' and add a 'Path' from the 'Blob stores' screen, referencing the new directory you created.
4. Click 'Create blob store'.

After the shared storage for blob stores is set up, continue to point all new repositories you create to the
shared location.

*Example: Configuring a Shared Blob Store for a Cluster*

Let's say you create a new blob store in an external backup location (e.g. `data-location`) and you want
to point a hosted npm repository to this location, for shared blob store access. Do the following:

1. Select a recipe from the 'Repositories' form, i.e. 'npm (hosted)'
2. Pick the `data-location` blob store in the 'Storage' section of the 'Repositories' form.
3. Click 'Create repository' to establish the new repository.

[[high-availability-nodes]]
==== Enabling High Availability

CAUTION: In the event you have empty nodes and are adding existing configured nodes to it, the existing 
unconfigured nodes would erase the existing configuration of the nodes added. When creating a cluster, it is 
important you start the configured nodes before the empty nodes to avoid unwanted configuration loss.

When you enable high availability, the nodes discover one another via link:https://hazelcast.com/[Hazelcast].
Hazelcast, by default, employs multicast to discover cluster members, but it supports node discovery in other
ways. If the default configuration isn't suitable for your network infrastructure, you will need to customize
`$install-dir/etc/fabric/hazelcast.xml`. See <<high-availability-aws>> for a concrete example.

Follow these steps to enable high availability:

1. In the first repository manager, open the `$data-dir/etc/nexus.properties` file.
2. Remove the `#` before +nexus.clustered = true+ to enable the node at start-up. 
3. Go to the second and third repository managers and repeat steps 1 and 2, to enable them for high availability.

[[high-availability-startup]]
==== Startup and Confirming Node Connectivity

After enabling high availability for your nodes, check the console to confirm that multicast discovers all three
corresponding nodes.

When you start the nodes, you will see a message in the `nexus.log` confirming the connection of the cluster
members, like the one below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [3] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702
    Member [192.168.99.1]:5703 this
}
----

[[high-availability-verify]]
==== Verifying Synchronization

At runtime, the repository manager user interface allows you to view the status of the nodes, regardless of
which you connect to, as they are synchronized.

See <<nodes>> for details on viewing active nodes in a cluster.

NOTE: In the event a single node loses connection to the cluster, the remaining nodes will continue to make
decisions on which data changes are valid. The disconnected node will reject further writes until it rejoins
the cluster.

[[high-availability-environment]]
==== Configuring a Cluster after Setup

Once you have your high availability environment set up, be aware that almost all configuration done via the 
user interface is shared among all nodes in the cluster. In an HA cluster all nodes are treated equally.
For example, if you create a new repository all nodes in the cluster will be able to see it and utilize it.
Or if you want to change your 'Email Server' port you just need to do it once via the user interface on any
of the servers and the change will share.

NOTE: Same as a single server be aware, if multiple people are configuring something at the same time in your 
cluster, it may appear the changes are not sharing. If you refresh your screen, the latest changes will appear.

There are some things, however, that are not done or shared within the UI and need to be done on each individual 
server. These include:

- Any configuration files you add or modify (such as specifying a port via `nexus.properties` or setting up SSL)
- 'Refresh Interval' of the UI 'Log Viewer' setting
- Most log messages are not shared across the server, however logging levels are shared
- 'Metrics' displayed are for the individual server
- A 'Support ZIP' is for the individual server. If you have issues forming a cluster, consult your support
technician and provide support zips for all nodes.

TIP: Scheduled tasks will run against one node unless the 'Multi node' configuration option is selected or the 
task affects something that is in itself shared (like compaction of blob stores).

Regardless, {nxrm} configuration should not be done through the cluster's load balancer. Configuration should
occur on the individual node level.

When adding new nodes to the existing cluster be aware that they will get the shared configuration of the cluster 
regardless of how they are preconfigured.

[[high-availability-monitor]]
==== Monitoring Node Health

Once your HA environment is set up, you should monitor the health of the nodes in your cluster. See the support
article for the https://support.sonatype.com/hc/en-us/articles/226254487[HTTP endpoints] that HA-C exposes and/or
the support article for https://support.sonatype.com/hc/en-us/articles/218501277[enabling JMX], if desired.

[[high-availability-aws]]
==== Configuring High Availability for Amazon Web Services

{nxrm} can be deployed on cloud-computing services, such as Amazon Web Services (AWS). Depending on your network
security, additional configuration may be required. For example, if you use a network layer firewall application
it may block multicast communication. If such a failure occurs you will need to modify the Hazelcast configuration
file.

To configure Hazelcast for automatic node discovery find the `<join>` tag in `$install-dir/etc/fabric/hazelcast.xml`.
Then, edit the file for each node:

1. Change the value in `<multicast enabled="true">` to `"false"`.
2. Change the value in `<aws enabled="false">` to `"true"`.
3. Save the file.
4. Reboot each node in the cluster.

The `$install-dir/etc/fabric/hazelcast.xml` file with the modified properties will look similar to this:
----
<join>
    <multicast enabled="false">
       <multicast-group>224.2.2.3</multicast-group>
       <multicast-port>54327</multicast-port>
    </multicast>
    <tcp-ip enabled="false">
        <interface>127.0.0.1</interface>
    </tcp-ip>
    <aws enabled="true">
        <access-key>my-access-key</access-key>
        <secret-key>my-secret-key</secret-key>
        <!--optional, default is us-east-1 -->
        <region>us-west-1</region>
        <!--optional, default is ec2.amazonaws.com. If set, region shouldn't be set as it will override this property -->
        <host-header>ec2.amazonaws.com</host-header>
        <!-- optional, only instances belonging to this group will be discovered, default will try all running instances -->
        <security-group-name>security-group-name</security-group-name>
        <tag-key>type</tag-key>
        <tag-value>nexus-nodes</tag-value>
    </aws>
</join>
----
